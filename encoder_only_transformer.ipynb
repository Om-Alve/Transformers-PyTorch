{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46399c7d-a332-4be8-b002-eb4891e35d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "device = 'cuda'\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # 32,87,40\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        # k -> 32,87,10\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        w = k @ q.transpose(-2, -1)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(-1).float() \n",
    "            w = w * attention_mask\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        out = w @ v\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,head_size,n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed,n_embed)\n",
    "    def forward(self,x,attention_mask):\n",
    "        out = torch.cat([head(x,attention_mask) for head in self.heads],-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed,n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(head_size,n_heads)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self,x,attention_mask):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.multihead(x,attention_mask)\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,n_embed)\n",
    "        self.positional_embedding = nn.Embedding(block_size,n_embed)\n",
    "        self.blocks = nn.ModuleList([Block() for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(n_embed)\n",
    "        self.cl_head = nn.Sequential(\n",
    "            nn.Linear(n_embed,6),\n",
    "        )\n",
    "    def forward(self,x,attention_mask,targets=None):\n",
    "        # b,t b=batch, t = tokens\n",
    "        B,T = x.shape\n",
    "        ini_emb = self.embedding(x)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T,device=device))\n",
    "        x = ini_emb + pos_emb\n",
    "        # b,t,c = 32,87,40\n",
    "        for block in self.blocks:\n",
    "            x = block(x,attention_mask)\n",
    "        x = self.ln(x)\n",
    "        x = self.cl_head(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b18dbc7-2701-4bd4-b314-cfe7b794d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e70329e-9e3c-47a5-a071-7ae082280032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training.csv',sep=',')\n",
    "X = df[\"text\"]\n",
    "y = np.array(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f17378-3447-44a0-9bb7-d5db7b45edab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer (change \"model_name\" to the appropriate model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode sentences and generate attention masks\n",
    "encoded_dict = tokenizer.batch_encode_plus(\n",
    "    list(X),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Retrieve encoded sentences and attention masks\n",
    "input_ids = encoded_dict['input_ids']\n",
    "attention_masks = encoded_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9231f931-3380-4729-b8c5-99452eb49719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_3148\\561058374.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(input_ids,device = device)\n",
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_3148\\561058374.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(attention_masks,device=device)\n"
     ]
    }
   ],
   "source": [
    "block_size = 87\n",
    "batch_size = 64\n",
    "n_embed = 60\n",
    "n_heads = 5\n",
    "head_size = 12\n",
    "n_layers = 2\n",
    "vocab_size = tokenizer.vocab_size\n",
    "X = torch.tensor(input_ids,device = device)\n",
    "y = torch.tensor(y, device = device)\n",
    "attention_masks = torch.tensor(attention_masks,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec55dd7-0ca7-4371-a102-980cf9f42c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batches(X, y, attention_masks, batch_size=32, num_batches=5):\n",
    "    idx = random.randint(0,len(X) - batch_size)\n",
    "    return X[idx:idx + batch_size],y[idx:idx + batch_size],attention_masks[idx:idx + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949f3625-bbfe-42e1-81a5-d1267f3f3917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8413, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.9070, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.3533, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0535, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0832, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model = Encoder(n_classes=6).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.006)\n",
    "\n",
    "for iter in range(1001):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb,ab = get_random_batches(X,y,attention_masks,batch_size=512)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model(xb,ab, F.one_hot(yb,num_classes=6))\n",
    "    loss = F.cross_entropy(logits,F.one_hot(yb,num_classes=6).type(torch.float32))\n",
    "    if iter % 100 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8a0cbf2-636a-4225-8136-f1be194902cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8733,  7.4193,  1.4417,  2.3786, -6.9307, -1.4388]],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = \"I'm not happy now i hate this\"\n",
    "xt = tokenizer(xtest,padding=\"max_length\",max_length=87,truncation=True)\n",
    "model(torch.tensor(xt['input_ids'],device=device).unsqueeze(0),torch.tensor(xt['attention_mask'],device=device).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49567025-8ad2-4dc1-b5b7-46829f83a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1001):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb,ab = get_random_batches(X,y,attention_masks,batch_size=128)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model(xb,ab, F.one_hot(yb,num_classes=6))\n",
    "    loss = F.cross_entropy(logits,F.one_hot(yb,num_classes=6).type(torch.float32))\n",
    "    if iter % 100 == 0:\n",
    "        print(loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5343f9-3fda-4601-a382-23919221237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0228, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb,ab = get_random_batches(X,y,attention_masks,batch_size=3000)\n",
    "\n",
    "    # evaluate the loss\n",
    "logits = model(xb,ab, F.one_hot(yb,num_classes=6))\n",
    "loss = F.cross_entropy(logits,F.one_hot(yb,num_classes=6).type(torch.float32))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1e6e4741-62b0-45bf-bafc-0652ea4065dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 5, 4, 1], dtype=int64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "236eda55-32aa-4475-99ac-2a8c27b15c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i do feel that running is a divine experience ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i have immense sympathy with the general point...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i do not feel reassured anxiety is on each side</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15986</th>\n",
       "      <td>i had a horrible horrible horrible time and ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15987</th>\n",
       "      <td>i feel energized but i find that i am much mor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15990</th>\n",
       "      <td>i feel really glad that i dont look like the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15993</th>\n",
       "      <td>i most days feel like if braeden and calvin ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>i feel strong and good overall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5362 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "8      i have been with petronas for years i feel tha...      1\n",
       "11     i do feel that running is a divine experience ...      1\n",
       "14     i have immense sympathy with the general point...      1\n",
       "15       i do not feel reassured anxiety is on each side      1\n",
       "22       i have the feeling she was amused and delighted      1\n",
       "...                                                  ...    ...\n",
       "15986  i had a horrible horrible horrible time and ho...      1\n",
       "15987  i feel energized but i find that i am much mor...      1\n",
       "15990  i feel really glad that i dont look like the c...      1\n",
       "15993  i most days feel like if braeden and calvin ar...      1\n",
       "15997                     i feel strong and good overall      1\n",
       "\n",
       "[5362 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.where(df['label'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78d6aa72-251e-4686-948b-e06cfaa3896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 3\n",
    "T = 3\n",
    "w = torch.rand((B,T,T))\n",
    "a = torch.tensor([[0,0,1],[0,1,0],[1,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14401825-4bf6-49fe-807a-22c0d85a8082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8488, 0.7205, 0.6193],\n",
       "         [0.8322, 0.6145, 0.1708],\n",
       "         [0.5892, 0.1883, 0.7754]],\n",
       "\n",
       "        [[0.0939, 0.6656, 0.2729],\n",
       "         [0.1989, 0.7057, 0.7418],\n",
       "         [0.5035, 0.7337, 0.3003]],\n",
       "\n",
       "        [[0.4917, 0.6449, 0.6367],\n",
       "         [0.1367, 0.6154, 0.2820],\n",
       "         [0.8568, 0.5544, 0.4145]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c4f6c08-f4cb-4ee2-a2ac-7c2a1ab7c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_3148\\3427253861.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(encoded_dict['input_ids'],device=device)\n",
      "C:\\Users\\omalv\\AppData\\Local\\Temp\\ipykernel_3148\\3427253861.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(encoded_dict['attention_mask'],device=device)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/validation.csv',sep=',')\n",
    "X_test = df[\"text\"]\n",
    "y_test = torch.tensor(df[\"label\"],device=device)\n",
    "\n",
    "encoded_dict = tokenizer.batch_encode_plus(\n",
    "    list(X_test),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Retrieve encoded sentences and attention masks\n",
    "input_ids = torch.tensor(encoded_dict['input_ids'],device=device)\n",
    "attention_masks = torch.tensor(encoded_dict['attention_mask'],device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1ee4e36-2098-48b0-846e-f79015f5e7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model(input_ids,attention_masks,y_test)\n",
    "F.cross_entropy(o,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ea856a0-97b0-46db-a357-a143a7100cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = a.unsqueeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e3593de-a3f7-4e93-84c3-4223f3b5cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_masked = w * mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6534eaa6-53da-454c-a70d-05a00b30b395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.5892, 0.1883, 0.7754]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [0.1989, 0.7057, 0.7418],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.4917, 0.6449, 0.6367],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36f2b05c-c339-4e12-8c6e-58e96c3024ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 87, 87])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec26f35-5e71-44e8-b9c1-16d812bbdfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
